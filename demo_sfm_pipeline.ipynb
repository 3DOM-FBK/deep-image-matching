{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RUN SfM Pipeline with Deep-Image-Matching\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deep Image Matching loaded in 10.615 seconds.\n"
          ]
        }
      ],
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "import deep_image_matching as dim\n",
        "\n",
        "logger = dim.logger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialization\n",
        "\n",
        "Get the list of possible pipelines and matching strategy and chose one of them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available pipelines:\n",
            "['superpoint+lightglue',\n",
            " 'superpoint+superglue',\n",
            " 'superpoint+kornia_matcher',\n",
            " 'disk+lightglue',\n",
            " 'aliked+lightglue',\n",
            " 'orb+kornia_matcher',\n",
            " 'sift+kornia_matcher',\n",
            " 'loftr',\n",
            " 'se2loftr',\n",
            " 'roma',\n",
            " 'keynetaffnethardnet+kornia_matcher',\n",
            " 'dedode+kornia_matcher',\n",
            " 'xfeat+kornia_matcher',\n",
            " 'sift+lightglue',\n",
            " 'dedode+lightglue',\n",
            " 'sift_pycolmap+lightglue']\n",
            "Available matching strategy:\n",
            "['bruteforce',\n",
            " 'sequential',\n",
            " 'retrieval',\n",
            " 'custom_pairs',\n",
            " 'matching_lowres',\n",
            " 'covisibility']\n"
          ]
        }
      ],
      "source": [
        "print(\"Available pipelines:\")\n",
        "pprint(dim.Config.get_pipelines())\n",
        "print(\"Available matching strategy:\")\n",
        "pprint(dim.Config.get_matching_strategies())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you have to build a dictionary with the input processing parameters (they are the same as the input parameters for the CLI) and pass it to the Config class to initialize the configuration object. Refer to the [documentation](https://3dom-fbk.github.io/deep-image-matching/) for more information about the parameters.\n",
        "\n",
        "Note that the `dir` defines the project directory, where the images are stored and the results will be saved.\n",
        "Deep-Image-Matching will search for the images inside an 'image' subdirectory and will save the results in a 'results\\_{processing_params}' subdirectory, where {processing_params} are some information on the processing parameters used.\n",
        "\n",
        "By default DIM will not run if the output directory already exists, to avoid overwriting previous results. If you want to overwrite the results, you can set the `force` parameter to True. We have not implemented the possibility to recover the previous results yet (e.g., by using existing extracted features), but we may add it in the future.\n",
        "\n",
        "The `config_file` parameter is the path to the configuration file (optional). In this file you can specify all the parameters that you need for controlling the feature extraction and matching. Refer to the [documentation](https://3dom-fbk.github.io/deep-image-matching/advanced_configuration/) for more information about how to write this file. Note that this file il optional, if you don't pass it, the default parameters will be used.\n",
        "\n",
        "If you use set `verbose` to True, DIM will log all the processing steps and it will save some figures with the extracted features and the matches in a `debug` folder inside the results directory. Note that this will slow down the processing and will create a lot of files if the dataset is big. It is reccomended to use it only for testing or debugging purposes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;33m2024-05-30 09:50:08 | [WARNING ] assets/example_cyprus/results_superpoint+lightglue_matching_lowres_quality_high already exists, but the '--force' option is used. Deleting the folder.\u001b[0m\n",
            "Using a custom configuration file: /home/francesco/phd/deep-image-matching/assets/example_cyprus/config_superpoint+lightglue.yaml\n"
          ]
        }
      ],
      "source": [
        "params = {\n",
        "    \"dir\": \"./assets/example_cyprus\",\n",
        "    \"pipeline\": \"superpoint+lightglue\",\n",
        "    \"config_file\": \"./assets/example_cyprus/config_superpoint+lightglue.yaml\",\n",
        "    \"strategy\": \"matching_lowres\",\n",
        "    \"quality\": \"high\",\n",
        "    \"tiling\": \"preselection\",\n",
        "    \"skip_reconstruction\": False,\n",
        "    \"force\": True,\n",
        "    \"camera_options\": \"./assets/example_cyprus/cameras.yaml\",\n",
        "    \"openmvg\": None,\n",
        "    \"verbose\": False,\n",
        "}\n",
        "config = dim.Config(params)\n",
        "\n",
        "# Save the configuration to a json file for later use\n",
        "config.save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can check the configuration object.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config general:\n",
            "{'camera_options': './assets/example_cyprus/cameras.yaml',\n",
            " 'config_file': None,\n",
            " 'db_path': None,\n",
            " 'dir': None,\n",
            " 'force': True,\n",
            " 'geom_verification': <GeometricVerification.PYDEGENSAC: 1>,\n",
            " 'global_feature': None,\n",
            " 'graph': True,\n",
            " 'gui': False,\n",
            " 'gv_confidence': 0.99999,\n",
            " 'gv_threshold': 4,\n",
            " 'image_dir': PosixPath('assets/example_cyprus/images'),\n",
            " 'images': None,\n",
            " 'matching_strategy': 'matching_lowres',\n",
            " 'min_inlier_ratio_per_pair': 0.25,\n",
            " 'min_inliers_per_pair': 10,\n",
            " 'min_matches_per_tile': 10,\n",
            " 'openmvg_conf': None,\n",
            " 'output_dir': PosixPath('assets/example_cyprus/results_superpoint+lightglue_matching_lowres_quality_high'),\n",
            " 'outs': None,\n",
            " 'overlap': None,\n",
            " 'pair_file': PosixPath('assets/example_cyprus/results_superpoint+lightglue_matching_lowres_quality_high/pairs.txt'),\n",
            " 'pipeline': None,\n",
            " 'preselection_pipeline': 'superpoint+lightglue',\n",
            " 'quality': <Quality.HIGH: 3>,\n",
            " 'retrieval': None,\n",
            " 'skip_reconstruction': False,\n",
            " 'strategy': 'matching_lowres',\n",
            " 'tile_overlap': 10,\n",
            " 'tile_preselection_size': 1000,\n",
            " 'tile_selection': <TileSelection.PRESELECTION: 3>,\n",
            " 'tile_size': (2400, 2000),\n",
            " 'try_match_full_images': False,\n",
            " 'upright': False,\n",
            " 'verbose': False}\n"
          ]
        }
      ],
      "source": [
        "print(\"Config general:\")\n",
        "pprint(config.general)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config extractor:\n",
            "{'fix_sampling': False,\n",
            " 'keypoint_threshold': 0.005,\n",
            " 'max_keypoints': 8000,\n",
            " 'name': 'superpoint',\n",
            " 'nms_radius': 4,\n",
            " 'remove_borders': 4}\n"
          ]
        }
      ],
      "source": [
        "print(\"Config extractor:\")\n",
        "pprint(config.extractor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config matcher:\n",
            "{'depth_confidence': 0.95,\n",
            " 'filter_threshold': 0.1,\n",
            " 'flash': True,\n",
            " 'mp': False,\n",
            " 'n_layers': 9,\n",
            " 'name': 'lightglue',\n",
            " 'width_confidence': 0.99}\n"
          ]
        }
      ],
      "source": [
        "print(\"Config matcher:\")\n",
        "pprint(config.matcher)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you want to change some parameters, you can do it by accessing the dictionary and changing the values. For example, if you want to change the number of features to extract, you can do it like this:\n",
        "\n",
        "```python\n",
        "config.extractor['max_keypoints'] = 1000\n",
        "```\n",
        "\n",
        "Note that different Extractors and Matchers have different parameters, so be careful when changing them.\n",
        "You can check the parameters of the Extractor and Matcher by accessing the corresponding dictionary, such as:\n",
        "\n",
        "```python\n",
        "print(dim.extractors.SuperPointExtractor.get_default_conf())\n",
        "print(dim.matchers.LightGlueMatcher.get_default_conf())\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the extraction and matching\n",
        "\n",
        "First, you have to create an instance of the ImageMatching class and pass the configuration object to it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded SuperPoint model\n",
            "Loaded SuperPoint model\n",
            "Loaded LightGlue model\n",
            "\u001b[0;37m2024-05-30 09:50:10 | [INFO    ] Running image matching with the following configuration:\u001b[0m\n",
            "\u001b[0;37m2024-05-30 09:50:10 | [INFO    ]   Image folder: assets/example_cyprus/images\u001b[0m\n",
            "\u001b[0;37m2024-05-30 09:50:10 | [INFO    ]   Output folder: assets/example_cyprus/results_superpoint+lightglue_matching_lowres_quality_high\u001b[0m\n",
            "\u001b[0;37m2024-05-30 09:50:10 | [INFO    ]   Number of images: 10\u001b[0m\n",
            "\u001b[0;37m2024-05-30 09:50:10 | [INFO    ]   Matching strategy: matching_lowres\u001b[0m\n",
            "\u001b[0;37m2024-05-30 09:50:10 | [INFO    ]   Image quality: HIGH\u001b[0m\n",
            "\u001b[0;37m2024-05-30 09:50:10 | [INFO    ]   Tile selection: PRESELECTION\u001b[0m\n",
            "\u001b[0;37m2024-05-30 09:50:10 | [INFO    ]   Feature extraction method: superpoint\u001b[0m\n",
            "\u001b[0;37m2024-05-30 09:50:10 | [INFO    ]   Matching method: lightglue\u001b[0m\n",
            "\u001b[0;37m2024-05-30 09:50:10 | [INFO    ]   Geometric verification: PYDEGENSAC\u001b[0m\n",
            "\u001b[0;37m2024-05-30 09:50:10 | [INFO    ]   CUDA available: True\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "matcher = dim.ImageMatcher(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Full Pipeline\n",
        "\n",
        "Then you can run the full pipeline (extraction and matching) by calling the `run` method.\n",
        "This method will automatically run all the steps needed to extract the features and match the images. It will return the path to the h5 files containing the features and the matches.\n",
        "\n",
        "The `features.h5` file contains the features extracted from each images, while the `matches.h5` file contains the indices of the features matched.\n",
        "\n",
        "You can decide to automatically save the results in a COLMAP database by setting the `export_to_colmap` parameter to True. The database will be saved in the results directory with the name `database.db`.\n",
        "If you set `verbose` to True (default is False), it will also save figures in the `debug` folder and print (a lot) more information on the process. Note that it may create quite a lot of files if the dataset is big, so it is recommended to use it only for testing or debugging purposes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded SuperPoint model\n",
            "\u001b[0;37m2024-05-30 09:50:10 | [INFO    ] Extracting features from downsampled images...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]/home/francesco/miniforge3/envs/dim/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0;37m2024-05-30 09:50:12 | [INFO    ] Matching downsampled images...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 45/45 [00:00<00:00, 51.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0;37m2024-05-30 09:50:13 | [INFO    ] Extracting features with superpoint...\u001b[0m\n",
            "\u001b[0;37m2024-05-30 09:50:13 | [INFO    ] superpoint configuration: \u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'fix_sampling': False,\n",
            " 'keypoint_threshold': 0.005,\n",
            " 'max_keypoints': 8000,\n",
            " 'name': 'superpoint',\n",
            " 'nms_radius': 4,\n",
            " 'remove_borders': 4}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:02<00:00,  3.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0;37m2024-05-30 09:50:16 | [INFO    ] Features extracted!\u001b[0m\n",
            "\u001b[0;37m2024-05-30 09:50:16 | [INFO    ] Matching features with lightglue...\u001b[0m\n",
            "\u001b[0;37m2024-05-30 09:50:16 | [INFO    ] lightglue configuration: \u001b[0m\n",
            "{'add_laf': False,\n",
            " 'add_scale_ori': False,\n",
            " 'depth_confidence': 0.95,\n",
            " 'descriptor_dim': 256,\n",
            " 'filter_threshold': 0.1,\n",
            " 'flash': True,\n",
            " 'input_dim': 256,\n",
            " 'mp': False,\n",
            " 'n_layers': 9,\n",
            " 'name': 'lightglue',\n",
            " 'num_heads': 4,\n",
            " 'scale_coef': 1.0,\n",
            " 'weights': None,\n",
            " 'width_confidence': 0.99}\n",
            "\u001b[0;37m2024-05-30 09:50:16 | [INFO    ] Matching features...\u001b[0m\n",
            "\u001b[0;37m2024-05-30 09:50:16 | [INFO    ] \u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 28/28 [00:13<00:00,  2.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0;37m2024-05-30 09:50:30 | [INFO    ] [Timer] | [matching] generate_pairs=4.992, extract_features=2.941, Match pair=0.614, Total execution=21.583\u001b[0m\n",
            "\u001b[0;37m2024-05-30 09:50:30 | [INFO    ] [Timer] | [Deep Image Matching] Total execution=0.000\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Run image matching\n",
        "feature_path, match_path = matcher.run(export_to_colmap=False, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Step-by-Step\n",
        "\n",
        "Alternatevely, you can run the process step-by-step to have more control on the process.\n",
        "\n",
        "First, generate pairs of images to be matched. This will create a file with the names of the imaes that will be matched together.\n",
        "The pairs generation is carried out based on the `strategy` parameter in the configuration object.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded SuperPoint model\n",
            "\u001b[0;37m2024-05-30 09:55:09 | [INFO    ] Extracting features from downsampled images...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 17.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0;37m2024-05-30 09:55:10 | [INFO    ] Matching downsampled images...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 45/45 [00:00<00:00, 50.60it/s]\n"
          ]
        }
      ],
      "source": [
        "pair_path = matcher.generate_pairs()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If your dataset has images with >40/50 deg rotations, it is suggested to rotate the images upright before doing the extraction and matching, which is particularly useful for deep-learning approaches that usually are not rotation invariant (e.g. SuperPoint).\n",
        "To rotate images so they will be all \"upright\":\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0;37m2024-05-30 09:55:48 | [INFO    ] Rotating images upright...\u001b[0m\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "Invalid config object. 'custom_config' must be a Config object",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotate_upright_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/phd/deep-image-matching/src/deep_image_matching/image_matching.py:257\u001b[0m, in \u001b[0;36mImageMatcher.rotate_upright_images\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    250\u001b[0m cv2_rot_params \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    252\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mROTATE_90_CLOCKWISE,\n\u001b[1;32m    253\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mROTATE_180,\n\u001b[1;32m    254\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mROTATE_90_COUNTERCLOCKWISE,\n\u001b[1;32m    255\u001b[0m ]\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotated_images \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 257\u001b[0m SPextractor \u001b[38;5;241m=\u001b[39m \u001b[43mSuperPointExtractor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgeneral\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mextractor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeypoint_threshold\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_keypoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m LGmatcher \u001b[38;5;241m=\u001b[39m LightGlueMatcher(\n\u001b[1;32m    267\u001b[0m     config\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneral\u001b[39m\u001b[38;5;124m\"\u001b[39m: {},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m     },\n\u001b[1;32m    275\u001b[0m )\n\u001b[1;32m    276\u001b[0m features \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeat0\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeat1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    279\u001b[0m }\n",
            "File \u001b[0;32m~/phd/deep-image-matching/src/deep_image_matching/extractors/superpoint.py:101\u001b[0m, in \u001b[0;36mSuperPointExtractor.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# Init the base class\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# Load extractor\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     SP_cfg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mextractor\n",
            "File \u001b[0;32m~/phd/deep-image-matching/src/deep_image_matching/extractors/extractor_base.py:47\u001b[0m, in \u001b[0;36mExtractorBase.__init__\u001b[0;34m(self, custom_config)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# If a custom config is passed, update the default config\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(custom_config, Config):\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid config object. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom_config\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a Config object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Update the default configuration of each specific extractor the with custom configation\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# TODO: this is not the best way to update the configuration, it should be improved\u001b[39;00m\n\u001b[1;32m     51\u001b[0m custom_config\u001b[38;5;241m.\u001b[39m_cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextractor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_conf, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcustom_config\u001b[38;5;241m.\u001b[39mextractor}\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid config object. 'custom_config' must be a Config object"
          ]
        }
      ],
      "source": [
        "matcher.rotate_upright_images()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the feature extraction on the images using the chosen extraction method. This will extract the features from the images and save them in the `features.h5` file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_path = matcher.extract_features()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the matching on the extracted features. This will match the features and save the matches in the `matches.h5` file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "match_path = matcher.match_pairs(feature_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If features have been extracted on \"upright\" images, this function bring features back to their original image orientation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "matcher.rotate_back_features(feature_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Access the results from the h5 files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can access the feautes and the matches from the h5 files using the h5py library. Here is an example on how to read the features and the matches from the h5 files:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<KeysViewHDF5 ['DSC_6466.JPG', 'DSC_6467.JPG', 'DSC_6468.JPG', 'DSC_6469.JPG', 'DSC_6470.JPG', 'DSC_6471.JPG', 'DSC_6472.JPG', 'DSC_6473.JPG', 'DSC_6474.JPG', 'DSC_6475.JPG']>\n",
            "<KeysViewHDF5 ['descriptors', 'image_size', 'keypoints', 'scores', 'tile_idx']>\n",
            "[[   2.  134.]\n",
            " [   2.  458.]\n",
            " [   5.  624.]\n",
            " ...\n",
            " [1492.  443.]\n",
            " [1492.  448.]\n",
            " [1493.  829.]]\n",
            "<KeysViewHDF5 ['descriptors', 'image_size', 'keypoints', 'scores', 'tile_idx']>\n",
            "[[   2.  894.]\n",
            " [   3.  867.]\n",
            " [   6.  273.]\n",
            " ...\n",
            " [1490.  749.]\n",
            " [1491.  436.]\n",
            " [1497.  147.]]\n"
          ]
        }
      ],
      "source": [
        "# The `features.h5` file\n",
        "\n",
        "import h5py\n",
        "\n",
        "images = matcher.image_list\n",
        "\n",
        "with h5py.File(feature_path, \"r\") as f:\n",
        "    print(f.keys())\n",
        "    print(f[images[0].name].keys())\n",
        "    print(f[images[0].name][\"keypoints\"][:])\n",
        "    print(f[images[1].name].keys())\n",
        "    print(f[images[1].name][\"keypoints\"][:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<KeysViewHDF5 ['DSC_6466.JPG', 'DSC_6467.JPG', 'DSC_6469.JPG', 'DSC_6470.JPG', 'DSC_6472.JPG', 'DSC_6473.JPG', 'DSC_6474.JPG']>\n",
            "<KeysViewHDF5 ['DSC_6467.JPG', 'DSC_6469.JPG', 'DSC_6470.JPG', 'DSC_6472.JPG', 'DSC_6473.JPG', 'DSC_6474.JPG', 'DSC_6475.JPG']>\n",
            "[[1007 3522]\n",
            " [1467 3787]\n",
            " [1679 2263]\n",
            " ...\n",
            " [4956 1903]\n",
            " [4957 1960]\n",
            " [4958 1832]]\n"
          ]
        }
      ],
      "source": [
        "# The `matches.h5` file\n",
        "\n",
        "with h5py.File(match_path, \"r\") as f:\n",
        "    print(f.keys())\n",
        "    g0 = f[images[0].name]\n",
        "    print(g0.keys())\n",
        "    g1 = g0[images[1].name]\n",
        "    print(g1.__array__())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export in colmap format\n",
        "\n",
        "Then you can use the `export_to_colmap` function that will read the features and the matches from the h5 files and will save them in a COLMAP sqlite database.\n",
        "DIM assigns camera models to images based on the options defined in `cameras.yaml` file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;33m2024-05-30 09:50:30 | [WARNING ] Was not possible to load the first image to initialize cam0\u001b[0m\n",
            "\u001b[1;33m2024-05-30 09:50:30 | [WARNING ] Was not possible to load the first image to initialize cam1\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 906.45it/s]\n",
            "28it [00:00, 6911.92it/s]             \n",
            "28it [00:00, 7246.28it/s]             \n"
          ]
        }
      ],
      "source": [
        "database_path = config.general[\"output_dir\"] / \"database.db\"\n",
        "dim.io.export_to_colmap(\n",
        "    img_dir=config.general[\"image_dir\"],\n",
        "    feature_path=feature_path,\n",
        "    match_path=match_path,\n",
        "    database_path=database_path,\n",
        "    camera_config_path=config.general[\"camera_options\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively you can assign camera models with a dictionary and pass the parameter `camera_config_dict` to the `export_to_colmap` function. This dictionary will override the camera models assigned in the `cameras.yaml` file.\n",
        "\n",
        "For images not assigned to specific `cam<x>` camera groups, the options specified under `general` are applied. The `camera_model` can be selected from `[\"simple-pinhole\", \"pinhole\", \"simple-radial\", \"opencv\"]`. It's worth noting that it's easily possible to extend this to include all the classical COLMAP camera models. Cameras can either be shared among all images (`single_camera == True`), or each camera can have a different camera model (`single_camera == False`).\n",
        "\n",
        "A subset of images can share intrinsics using `cam<x>` key, by specifying the `camera_model` along with the names of the images separated by commas. There's no limit to the number of `cam<x>` entries you can use.\n",
        "\n",
        "**Note**: Use the SIMPLE-PINHOLE camera model if you want to export the solution to Metashape later, as there are some bugs in COLMAP (or pycolamp) when exportingthe solution in the Bundler format.\n",
        "e.g., using FULL-OPENCV camera model, the principal point is not exported correctly and the tie points are wrong in Metashape.\n",
        "\n",
        "```python\n",
        "camera_config_dict = {\n",
        "    \"general\": {\n",
        "        \"camera_model\": \"pinhole\",  # [\"simple-pinhole\", \"pinhole\", \"simple-radial\", \"opencv\"]\n",
        "        \"single_camera\": True,\n",
        "    },\n",
        "    \"cam0\": {\n",
        "        \"camera_model\": \"pinhole\",\n",
        "        \"images\": \"DSC_6468.JPG,DSC_6468.JPG\",\n",
        "    },\n",
        "    \"cam1\": {\n",
        "        \"camera_model\": \"pinhole\",\n",
        "        \"images\": \"\",\n",
        "    },\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run reconstruction\n",
        "\n",
        "You can run the reconstruction with pycolmap, OpenMVG or MICMAC. The suggested method is pycolmap, as it is the most integrated with DIM.\n",
        "\n",
        "Note that pycolmap is not installed by default with DIM, so you have to install it manually. You can find the installation instructions [here](https://3dom-fbk.github.io/deep-image-matching/installation/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import pycolmap\n",
        "except ImportError:\n",
        "    logger.error(\"Pycomlap is not available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the pycolmap module is imported, you can define all the parameters for the COLAMP reconstruction.\n",
        "You can check all the available parameters with:\n",
        "\n",
        "```python\n",
        "print(pycolmap.IncrementalPipelineOptions().summary())\n",
        "```\n",
        "\n",
        "Alternatevely, you can leave the dictionary empty for using the default confiuration\n",
        "\n",
        "```python\n",
        "reconst_opts = {}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run reconstruction\n",
        "opt = dict(\n",
        "    triangulation=dict(\n",
        "        ignore_two_view_tracks=False,\n",
        "        min_angle=0.5,\n",
        "    ),\n",
        "    mapper=dict(filter_min_tri_angle=0.5, filter_max_reproj_error=5.0),\n",
        ")\n",
        "refine_intrinsics = False\n",
        "verbose = False\n",
        "\n",
        "model = dim.reconstruction.pycolmap_reconstruction(\n",
        "    database_path=config.general[\"output_dir\"] / \"database.db\",\n",
        "    sfm_dir=config.general[\"output_dir\"],\n",
        "    image_dir=config.general[\"image_dir\"],\n",
        "    refine_intrinsics=refine_intrinsics,\n",
        "    options=opt,\n",
        "    verbose=verbose,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print COLMAP camera values\n",
        "print(list(model.cameras.values()))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "deep_image_matching",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
